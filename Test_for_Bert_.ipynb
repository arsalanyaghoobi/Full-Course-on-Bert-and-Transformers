{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test for Bert .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0skNaUu1A1fXimXiIs1AS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arsalanyaghoobi/Full-Course-on-Bert-and-Transformers/blob/main/Test_for_Bert_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hnx4KzMnEU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba64ff2b-fed6-4648-88f9-de0ca60ddaa1"
      },
      "source": [
        "import tensorflow as tf     # TensorFlow is a library which can be applied to all the machine learning algorithms especially deep learning with neural network.\n",
        "device_name= tf.test.gpu_device_name()\n",
        "print(device_name)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI9bNAWJXTt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dc1e7e5-fc16-4703-90f5-c674a309fcd5"
      },
      "source": [
        "!pip install transformers tensorflow_datasets     # Provides thousands of pretrained models\n",
        "                                                  # provides APIs to quickly download and use those pretrained models\n",
        "                                                  #Transformers is backed by the two most popular deep learning libraries, PyTorch and TensorFlow"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (4.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.24.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.3.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.3.3)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (20.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.52.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTdLlZqKYHZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a86d0a-cbce-4f98-9405-28a0fef2bfc2"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "(ds_train, ds_test),ds_info=tfds.load(\"imdb_reviews\",#tfds.load\n",
        "                                    split=(tfds.Split.TRAIN,tfds.Split.TEST),\n",
        "                                    as_supervised=True,\n",
        "                                    with_info=True)\n",
        "print(ds_train,ds_test,ds_info)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Load dataset info from /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n",
            "INFO:absl:Reusing dataset imdb_reviews (/root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split (Split('train'), Split('test')), from /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)> <PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)> tfds.core.DatasetInfo(\n",
            "    name='imdb_reviews',\n",
            "    version=1.0.0,\n",
            "    description='Large Movie Review Dataset.\n",
            "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
            "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
            "    features=FeaturesDict({\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
            "        'text': Text(shape=(), dtype=tf.string),\n",
            "    }),\n",
            "    total_num_examples=100000,\n",
            "    splits={\n",
            "        'test': 25000,\n",
            "        'train': 25000,\n",
            "        'unsupervised': 50000,\n",
            "    },\n",
            "    supervised_keys=('text', 'label'),\n",
            "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
            "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
            "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
            "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
            "      month     = {June},\n",
            "      year      = {2011},\n",
            "      address   = {Portland, Oregon, USA},\n",
            "      publisher = {Association for Computational Linguistics},\n",
            "      pages     = {142--150},\n",
            "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
            "    }\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhQ5gbVec5V_"
      },
      "source": [
        "1)Positional arguments must appear before a keyword argument in Python. This is because Python interprets positional arguments in the order in which they appear. Then, it interprets the keyword arguments that have been specified.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q3ObYhgo1su",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16872e49-4a7b-4646-b80c-9e9fd399fa22"
      },
      "source": [
        "for review, label in tfds.as_numpy(ds_train.take(100)): ##### not neccesary step\n",
        "  print(\"review\", review[0:5],\"Label\", label)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "review b'This ' Label 0\n",
            "review b'I hav' Label 0\n",
            "review b'Mann ' Label 0\n",
            "review b'This ' Label 1\n",
            "review b'As ot' Label 1\n",
            "review b'This ' Label 1\n",
            "review b'Okay,' Label 0\n",
            "review b'The f' Label 0\n",
            "review b'I rea' Label 0\n",
            "review b'Sure,' Label 0\n",
            "review b'Durin' Label 0\n",
            "review b'Cute ' Label 1\n",
            "review b'This ' Label 1\n",
            "review b'Put t' Label 0\n",
            "review b'Hilar' Label 1\n",
            "review b'It wa' Label 0\n",
            "review b'This ' Label 1\n",
            "review b'Final' Label 1\n",
            "review b'India' Label 1\n",
            "review b'Natha' Label 0\n",
            "review b'I can' Label 1\n",
            "review b'In a ' Label 1\n",
            "review b'A pre' Label 1\n",
            "review b'Excep' Label 1\n",
            "review b'In th' Label 1\n",
            "review b'Well,' Label 0\n",
            "review b'I act' Label 0\n",
            "review b'I lik' Label 0\n",
            "review b'My ma' Label 1\n",
            "review b'This ' Label 0\n",
            "review b'Mike ' Label 0\n",
            "review b'(Hone' Label 0\n",
            "review b'The \"' Label 0\n",
            "review b'I was' Label 0\n",
            "review b\"I'm s\" Label 0\n",
            "review b'A mov' Label 1\n",
            "review b'Of th' Label 1\n",
            "review b'\"Wild' Label 0\n",
            "review b'This ' Label 0\n",
            "review b'This ' Label 1\n",
            "review b'I hav' Label 0\n",
            "review b'Altho' Label 0\n",
            "review b'One o' Label 0\n",
            "review b'I was' Label 0\n",
            "review b'I rea' Label 0\n",
            "review b'The d' Label 0\n",
            "review b'Luise' Label 0\n",
            "review b'I tho' Label 1\n",
            "review b'This ' Label 0\n",
            "review b'\"The ' Label 1\n",
            "review b'Not t' Label 1\n",
            "review b'Only ' Label 1\n",
            "review b'I jus' Label 1\n",
            "review b'I kno' Label 1\n",
            "review b'How D' Label 0\n",
            "review b'I was' Label 1\n",
            "review b'As th' Label 0\n",
            "review b'Elega' Label 1\n",
            "review b'Havin' Label 1\n",
            "review b'Calli' Label 1\n",
            "review b'The b' Label 1\n",
            "review b'A hil' Label 1\n",
            "review b'Judgi' Label 0\n",
            "review b'I did' Label 1\n",
            "review b'Lisa ' Label 1\n",
            "review b'Anoth' Label 1\n",
            "review b'What ' Label 0\n",
            "review b'I abs' Label 0\n",
            "review b'This ' Label 1\n",
            "review b'I had' Label 0\n",
            "review b'First' Label 0\n",
            "review b'Corky' Label 0\n",
            "review b'From ' Label 1\n",
            "review b'What ' Label 0\n",
            "review b'The T' Label 1\n",
            "review b'I hav' Label 1\n",
            "review b'I jus' Label 1\n",
            "review b'In an' Label 1\n",
            "review b'No ,I' Label 0\n",
            "review b'I hav' Label 1\n",
            "review b'An aw' Label 0\n",
            "review b'I had' Label 0\n",
            "review b'Get t' Label 1\n",
            "review b'Bromw' Label 1\n",
            "review b'This ' Label 0\n",
            "review b'This ' Label 0\n",
            "review b'I rea' Label 1\n",
            "review b'There' Label 1\n",
            "review b'As St' Label 0\n",
            "review b'I fir' Label 1\n",
            "review b'Not o' Label 0\n",
            "review b'...Or' Label 0\n",
            "review b'Somet' Label 1\n",
            "review b'this ' Label 1\n",
            "review b'Are y' Label 0\n",
            "review b'I hav' Label 1\n",
            "review b'Carlo' Label 1\n",
            "review b'Why d' Label 1\n",
            "review b\"I've \" Label 0\n",
            "review b'My fa' Label 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXn8Jgqjt7kE"
      },
      "source": [
        "1)In the section above we called for review and label.\n",
        "\n",
        "2)we used `tfds.as_numpy()` in order to turn the data into numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY1ELCLZrm6g"
      },
      "source": [
        "from transformers import BertTokenizer        #import BertTokenizer\n",
        "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')    #choose the type of tokenizer(https://huggingface.co/transformers/pretrained_models.html)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziYBTU5w-xna"
      },
      "source": [
        "In order to use BERT text embeddings as input to train text classification model, we need to tokenize our text reviews. Tokenization refers to dividing a sentence into individual words.\n",
        "The BERT tokenizer use WordPiece vocabulary.It has over 30000 words and it maps pretrained embeddings for each.Each word has its own IDs; we would need to map the tokens to those IDs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy0sFRJq-3fg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d652614-877c-483f-a066-ba7be9067fb0"
      },
      "source": [
        "vocabulary= tokenizer.get_vocab()       # tokenize our text reviews; why dataset name is not mentioned in the paranthesis?\n",
        "vocabulary_list=list(vocabulary.keys())\n",
        "print(vocabulary_list[4000])\n",
        "print(len(vocabulary))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tears\n",
            "30522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxH5SSE3I5GF"
      },
      "source": [
        "In order to be able to enlist the vocabularies and make them all callable,\n",
        "`vocab_list=list(vocabulary.keys())`\n",
        "is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybypftgzBYt4"
      },
      "source": [
        "max_length_test= 50\n",
        "test_sentence='test tokenization sentence by Arsalan.Followed by another sentence'"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcXawqJnFv83"
      },
      "source": [
        "`Becareful this section on , you only will experinece processing your test data and not the training.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSisfSi_BmeD"
      },
      "source": [
        "Adding Special Token (CLS tokens are always at the beginning of the sentence and SEP tokens always at the end).\n",
        "how an input sentence should be represented in BERT. BERT embeddings are trained with two training tasks:\n",
        "\n",
        "Classification Task: to determine which category the input sentence should fall into\n",
        "Next Sentence Prediction Task: to determine if the second sentence naturally follows the first sentence.\n",
        "\n",
        "Sum Up:`to preprocess the input text data, the first thing we will have to do is to add the [CLS] token at the beginning, and the [SEP] token at the end of each input text.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQvehPJJKJib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c83a37b-64e8-4148-c340-efdda1e2eea9"
      },
      "source": [
        "test_sentence_with_special_tokens='[CLS]'+ test_sentence +'[SEP]'\n",
        "tokenized_test_sentence_with_special_tokens=tokenizer.tokenize(test_sentence_with_special_tokens)\n",
        "print(tokenized_test_sentence_with_special_tokens)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'test', 'token', '##ization', 'sentence', 'by', 'ars', '##ala', '##n', '.', 'followed', 'by', 'another', 'sentence', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt1dRmwUh2bv"
      },
      "source": [
        "\n",
        "\n",
        "Convert Tokens to IDs in WordPiece"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lAOa8zWMuz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e429cfcc-f9ec-445c-de01-1e0430157fa6"
      },
      "source": [
        "input_ids = tokenizer.convert_tokens_to_ids(tokenized_test_sentence_with_special_tokens)\n",
        "print(input_ids)\n",
        "print(len(input_ids))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 3231, 19204, 3989, 6251, 2011, 29393, 7911, 2078, 1012, 2628, 2011, 2178, 6251, 102]\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV08tSk2QdUd"
      },
      "source": [
        "Every time you have an input ID a attentionmask is required.The attention mask is a binary tensor `indicating the position of the padded indices` so that the model does not attend to them. For the BertTokenizer, **1 indicates a value that should be attended to, while 0 indicates a padded value**. This attention mask is in the dictionary returned by the tokenizer under the key “attention_mask”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChG-onsrQe2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865f6634-d94e-46be-8861-fad443198d88"
      },
      "source": [
        "attention_mask = [1] * len(input_ids)    #why should we have attention mask? Attention mask is required after the pooling!!!! the answer is clear: we only calculate the mask value ...\n",
        "print(attention_mask)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mugVHWbDjKYd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac04f1f-7846-4613-fe7b-87db723ea9af"
      },
      "source": [
        "padding_length = max_length_test - len(input_ids)  # calculating pad length\n",
        "print(padding_length)\n",
        "input_ids= input_ids + ([0] * padding_length)      # input ID = input ID +(Number of pad characters * [0])\n",
        "print(input_ids)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35\n",
            "[101, 3231, 19204, 3989, 6251, 2011, 29393, 7911, 2078, 1012, 2628, 2011, 2178, 6251, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBzHrkbKyWO4"
      },
      "source": [
        "Because you updated your input_ID it is required to update you attention mask as well;accordingly you wont pay attention to your padded tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnr9CZv1y8bL"
      },
      "source": [
        "attention_mask=(attention_mask + [0]*padding_length) #attention mask as been calculated above = [1]* len(input_ids)\n",
        "                                                     # at this satge we want to impose it on ids, so : [1]*len(input_ids) + [0]*padding_length"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES3PtYtj0Yb4"
      },
      "source": [
        "In the next step `token_type_ids` will differentiate consequent sequences from eachother by identifying IDs of a sequent by 0 and the next one by 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP1mhiVY194C"
      },
      "source": [
        "token_type_ids =[0]*max_length_test  # bcause the largest sequence of our dataset is max_length, thus ,in order to differentiate first and second sequences from each other we multiplied\n",
        "                                     # max_length into [0] and [1], so that we will access to both. On the other hand if you only need one of the sentences do not multiply by [1]."
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awm0r9ULIrlS"
      },
      "source": [
        "it is used for defining the kind of input including distinguishing question from answer, distinguishing causes and effects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvfarquo4XE5"
      },
      "source": [
        "defining a bert input, it is needed to define three elements:\n",
        "1)Token ids\n",
        "2)Token Type ids\n",
        "3)attention mask\n",
        "\n",
        "DONT FORGET that dataset gets into bert token by token. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPaDuyhxyw4W"
      },
      "source": [
        "`BEFORE ENCODING IT IS NEEDED TO MAKE A DICTIONARY LIST`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4D6lgJk2Nk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "271d9f2c-bdb4-4fc3-af89-ccc9dfecc368"
      },
      "source": [
        "bert_input={\n",
        "    \"token_ids\":input_ids,\n",
        "    \"token_type_ids\":token_type_ids,\n",
        "    \"attention_mask\":attention_mask\n",
        "}\n",
        "print(bert_input)       # The required items to define Bert input features are token_ids, token_type_ids and attention_mask (Attention : data is converted into dictiionary)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'token_ids': [101, 3231, 19204, 3989, 6251, 2011, 29393, 7911, 2078, 1012, 2628, 2011, 2178, 6251, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydGjMoBM6mXJ"
      },
      "source": [
        "In the sections above, we received the data as sentence and then we turned it into tokenz;we maped the tokens to corresponding IDs using(tokenizer and convert_token_to_ID);in the section below, we use encode plus as a substitute for all mentioned above.This method adds special tokens auomatically.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJHr59IA7Q9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fed0ad9-298c-40b3-9261-329283435c3f"
      },
      "source": [
        "bert_input= tokenizer.encode_plus(\n",
        "    test_sentence,\n",
        "    max_length_test=max_length_test,\n",
        "    add_special_tokens=True,\n",
        "    pad_to_max_length=True,\n",
        "    return_attention_mask=True\n",
        ")\n",
        "print(bert_input)         ##### Optional "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Keyword arguments {'max_length_test': 50} not recognized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [101, 3231, 19204, 3989, 6251, 2011, 29393, 7911, 2078, 1012, 2628, 2011, 2178, 6251, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHBNtQfKGIny"
      },
      "source": [
        "`DONT FORGET , THE INPUT TO ENCODE_PLUS WAS ONLY YOUR TEST DATASET AND NOT TRAINING.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N56Xez918vya"
      },
      "source": [
        "**Input IDs:** The input IDs are often the only required parameters to be passed to the model as input.They are token indices, numerical representations of tokens building the sequences that will be used as input by the model.\n",
        "\n",
        "\n",
        "**Attention Mask:** using Mask inorder to avoid attention on the **Pad Token Indices**. Mask values are selected in a range of [0,1]; 1 for tokens that are not masked and 0 for masked tokens.\n",
        "\n",
        "\n",
        "**Token Type IDs:**Some models purpose is to do sequence classification or question answering.These required two different sequences to be encoded in the same input IDs.They are usually seperated by special tokens, such as classifiers and separator tokens.for example the bert model builds its two sequence input as such:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIf-jb_fxHRH"
      },
      "source": [
        "max_length = 512 #max length for bert can be up to 512\n",
        "batch_size = 6 #recommended batch size for bert is 16,32,... how ever on \n",
        "               #this dataset we are overfitting quiet often and smaller batch size works like regularization.\n",
        "               #You might play with adding another dropout layer instead."
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBm-PAmGEgko"
      },
      "source": [
        "Encode Training and Test Dataset\n",
        "\n",
        "\n",
        "Now lets combine the whole encoding process to one function;` so that we can map over our train and test data`.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFwDsCN5VwsQ"
      },
      "source": [
        "def convert_example_to_feature(review):   #why is it needed to do so? what if we did not use encode_plus? YOU HAVE TO DO ALL YOU DID TO TEST SENTENCE THE SAME TO ALL YOUR DATASET.\n",
        "  return tokenizer.encode_plus(review,\n",
        "                               max_length=max_length,\n",
        "                               add_special_tokens=True,\n",
        "                               pad_to_max_length=True,\n",
        "                               return_attention_mask=True\n",
        ")"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04yRcrMCG2uz",
        "outputId": "45505e44-7ac8-4d05-c8bf-910d863b9826"
      },
      "source": [
        "print(bert_input)\n",
        "# Bert input is including input_ids, token_type_ids , attention_mask; as presented below they are all numbers which which should be turned into dictionaries so that can be finally turned into \n",
        "#lists and by using append all the slices shoul b eturned into one sequence."
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [101, 3231, 19204, 3989, 6251, 2011, 29393, 7911, 2078, 1012, 2628, 2011, 2178, 6251, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8qj-goZyOK0"
      },
      "source": [
        "`BEFORE ENCODING IT IS NEEDED TO MAKE A DICTIONARY LIST`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbROrcy4lKzR"
      },
      "source": [
        "# lets map to the expected input to TFBertForSequenceClassification, see here\n",
        "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
        "  return{\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\":token_type_ids,\n",
        "      \"attention_mask\":attention_masks,\n",
        "\n",
        "  }, label\n",
        "\n",
        "    # what are these limitations?\n",
        "def encode_examples(ds, limit=-1):\n",
        "    #preparing a list of all features of Bert_input so that we can build up final \n",
        "    #tensorflow dataset from slices.\n",
        "    input_ids_list = []\n",
        "    token_type_ids_list= []\n",
        "    attention_mask_list= []\n",
        "    label_list = []\n",
        "    # what are these limitations?\n",
        "    if(limit > 0):\n",
        "      ds=ds.take(limit)\n",
        "\n",
        "\n",
        "    for review, label in tfds.as_numpy(ds):\n",
        "\n",
        "        bert_input = convert_example_to_feature(review.decode())\n",
        "\n",
        "        input_ids_list.append(bert_input[\"input_ids\"])\n",
        "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
        "        attention_mask_list.append(bert_input['attention_mask'])\n",
        "        label_list.append([label])\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices((input_ids_list,attention_mask_list,token_type_ids_list,label_list)).map(map_example_to_dict)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IT6qWtU-Mqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97c0398f-8524-43a1-bc88-27c9129e135e"
      },
      "source": [
        "ds_train_encoded = encode_examples(ds_train).shuffle(10000).batch(batch_size)\n",
        "ds_test_encoded = encode_examples(ds_test).batch(batch_size)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAnGpSswNu7e",
        "outputId": "ab26199c-85a3-4452-86cc-16018c0cc1d6"
      },
      "source": [
        "from transformers import TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "model =TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier', 'dropout_189']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_werfeEPsB9"
      },
      "source": [
        "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits= True)\n",
        "metrics =tf.keras.metrics.TopKCategoricalAccuracy(k=1)\n",
        "optimizer = 'adam'\n",
        "model.compile(optimizer , loss , metrics=[metrics])"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrDvUYnhWRIl",
        "outputId": "8e3097f5-84ed-4569-bbaf-6fd1d0493958"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  109482240 \n",
            "_________________________________________________________________\n",
            "dropout_189 (Dropout)        multiple                  0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  1538      \n",
            "=================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4jjNVyEWfHY"
      },
      "source": [
        "learning_rate = 2e-5\n",
        "number_of_epoches =1\n",
        "bert_history = model.fit(ds_train_encoded, epochs=number_of_epoches,validation_data = ds_test_encoded)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}